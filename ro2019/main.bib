
@article{mcphillips2015yesworkflowa,
  title = {\href{https://doi.org/10.2218/ijdc.v10i1.370}{{{YesWorkflow}}: {{A User}}-{{Oriented}}, {{Language}}-{{Independent Tool}} for {{Recovering Workflow Information}} from {{Scripts}}}},
  volume = {10},
  abstract = {Scientific workflow management systems offer features for composing complex computational pipelines from modular building blocks, executing the resulting automated workflows, and recording the provenance of data products resulting from workflow runs. Despite the advantages such features provide, many automated workflows continue to be implemented and executed outside of scientific workflow systems due to the convenience and familiarity of scripting languages (such as Perl, Python, R, and MATLAB), and to the high productivity many scientists experience when using these languages. YesWorkflow is a set of software tools that aim to provide such users of scripting languages with many of the benefits of scientific workflow systems. YesWorkflow requires neither the use of a workflow engine nor the overhead of adapting code to run effectively in such a system. Instead, YesWorkflow enables scientists to annotate existing scripts with special comments that reveal the computational modules and dataflows otherwise implicit in these scripts. YesWorkflow tools extract and analyze these comments, represent the scripts in terms of entities based on the typical scientific workflow model, and provide graphical renderings of this workflow-like view of the scripts. Future version of YesWorkflow will also allow the prospective provenance of the data products of these scripts to be queried in ways similar to those available to users of scientific workflow systems.},
  journal = {Intl\. J.\ of Digital Curation},
  url = {http://www.ijdc.net/index.php/ijdc/article/view/10.1.298},
  author = {McPhillips, Timothy and Song, Tianhong and Kolisnik, Tyler and Aulenbach, Steve and Belhajjame, Khalid and Bocinsky, R. Kyle and Cao, Yang and Cheney, James and Chirigati, Fernando and Dey, Saumen and Freire, Juliana and Jones, Christopher and Hanken, James and Kintigh, Keith W. and Kohler, Timothy A. and Koop, David and Macklin, James A. and Missier, Paolo and Schildhauer, Mark and Schwalm, Christopher and Wei, Yaxing and Bieda, Mark and Lud\"ascher, Bertram},
  year = {2015},
  pages = {298-313},
}

@article{brinckman2019computing,
  title = {\href{https://doi.org/10.1016/j.future.2017.12.029}{Computing Environments for Reproducibility: {{Capturing}} the ``{{Whole Tale}}''}},
  volume = {94},
  issn = {0167-739X},
  shorttitle = {Computing Environments for Reproducibility},
  doi = {10.1016/j.future.2017.12.029},
  abstract = {The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale~project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process\textemdash{}transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create ``living publications'' or tales. The Whole Tale~focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale~environment.},
  urldate = {2019-02-18},
  journal = {FGCS},
  url = {http://www.sciencedirect.com/science/article/pii/S0167739X17310695},
  author = {Brinckman, Adam and Chard, Kyle and Gaffney, Niall and Hategan, Mihael and Jones, Matthew B. and Kowalik, Kacper and Kulasekaran, Sivakumar and Lud\"ascher, Bertram and Mecum, Bryce D. and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian J. and Turk, Matthew J. and Turner, Kandace},
  year = {2019},
  keywords = {Provenance,Code sharing,Data sharing,Living publications,Reproducibility},
  pages = {854-867},
  file = {/Users/ludaesch/Dropbox/zotero/storage/KLFKJKA6/Brinckman et al. - 2019 - Computing environments for reproducibility Captur.pdf;/Users/ludaesch/Dropbox/zotero/storage/KUSYCIYQ/Brinckman et al. - 2018 - Computing environments for reproducibility Captur.pdf;/Users/ludaesch/Dropbox/zotero/storage/J6LW73BC/S0167739X17310695.html;/Users/ludaesch/Dropbox/zotero/storage/TQXF6RVN/S0167739X17310695.html}
}




@book{verborgh_using_2013,
	address = {Birmingham Mumbai},
	series = {Community experience distilled},
	title = {Using {OpenRefine}},
	isbn = {978-1-78328-908-0},
	shorttitle = {Using {OpenRefine}},
	language = {en},
	publisher = {Packt Publishing},
	author = {Verborgh, Ruben and De Wilde, Max},
	year = {2013},
	file = {Verborgh and De Wilde - 2013 - Using OpenRefine the essential OpenRefine guide t.pdf:/Users/tmcphill/Zotero/storage/QPSPISA9/Verborgh and De Wilde - 2013 - Using OpenRefine the essential OpenRefine guide t.pdf:application/pdf}
}

@Misc{makepeace18ORclient,
  author = 	 {Paul Makepeace and Felix Lohmeier},
  title = 	 {{OpenRefine} {Python} Client Library},
  howpublished = {\url{https://github.com/opencultureconsulting/openrefine-client}},
  year = 	 2018}

@Misc{docker2019,
  title = 	 {{Docker} Documentation},
  howpublished = {\url{https://docs.docker.com/}},
  year = 	 2019}

@Misc{jupyter2019,
  title = 	 {Project {Jupyter}},
  howpublished = {\url{https://jupyter.org/}},
  year = 	 2019}

@Misc{Singularity2019,
  title = 	 {Singularity},
  howpublished = {\url{https://singularity.lbl.gov/}},
  year = 	 2019}




@Misc{Binder_2018,
  author = 	 {Jupyter-Project},
  title = 	 {\href{https://doi.org/10.25080/Majora-4af1f417-011}{Binder 2.0 - Reproducible, Interactive, Sharable Environments for Science at Scale}},
  howpublished = {17th Python in Science Conference},
  year = 	 2018}

@Misc{OpenRefine,
  key = 	 {OR},
  title = 	 {{OpenRefine}: A free, open source, powerful tool for working with messy data},
  year = 	 2018,
  howpublished = 	 {\url{http://openrefine.org/}}}

@Misc{QEMU,
  key = 	 {QEMU},
  title = 	 {{QEMU}: the FAST! processor emulator},
  year = 	 2019,
  howpublished = 	 {\url{https://www.qemu.org/}}}

@Misc{yw-website,
key = {YW},
title = {\textsf{YesWorkflow} project site and README},
howpublished = {\href{http://yesworkflow.org/yw-prototypes}{yesworkflow.org/yw-prototypes}},
year=2018
}

@InProceedings{mcphillips2015ywIDCC,
  author    = {Timothy M. McPhillips and
               Tianhong Song and
               Tyler Kolisnik and
               Steve Aulenbach and
               Khalid Belhajjame and
               Kyle Bocinsky and
               Yang Cao and
               Fernando Chirigati and
               Saumen C. Dey and
               Juliana Freire and
               Deborah N. Huntzinger and
               Christopher Jones and
               David Koop and
               Paolo Missier and
               Mark Schildhauer and
               Christopher R. Schwalm and
               Yaxing Wei and
               James Cheney and
               Mark Bieda and
               Bertram Lud{\"{a}}scher},
  title     = {\textsf{YesWorkflow}: {A} User-Oriented, Language-Independent Tool for Recovering
               Workflow Information from Scripts},
  booktitle   = {10th Intl.\ Digital Curation Conference (IDCC)},
    year      = 2015,
  month = {February},
  address = {London},
  note       = {to appear in IJDC. Preprint: \href{http://arxiv.org/abs/1502.02403}{arxiv.org/abs/1502.02403}},
}

@Misc{xsb_software,
  key = 	 {XSB Group},
  title = 	 {\href{https://sourceforge.net/projects/xsb/}{The XSB logic programming system, version 3.7}},
  year = 	 2017,
}

@Misc{openrefine-provenance-repo,
    key = {openrefine-provenance-repo},
    howpublished = {\href{https://github.com/idaks/openrefine-provenance}{https://github.com/idaks/openrefine-provenance}}
 }

@Misc{openrefine-reproducibility-repo,
    key = {openrefine-reproducibility-repo},
    howpublished = {\href{https://github.com/idaks/openrefine-reproducibility}{https://github.com/idaks/openrefine-reproducibility}}
  }
  






@article{gryk2017workflows,
  title = {Workflows and {{Provenance}}: {{Toward Information Science Solutions}} for the {{Natural Sciences}}},
  volume = {65},
  issn = {1559-0682},
  shorttitle = {Workflows and {{Provenance}}},
  abstract = {{$<$}p{$>$}The era of big data and ubiquitous computation has brought with it concerns about ensuring reproducibility in this new research environment. It is easy to assume that computational methods self-document by their very nature of being exact, deterministic processes. However, similar to laboratory experiments, ensuring reproducibility in the computational realm requires the documentation of both the protocols used (workflows), as well as a detailed description of the computational environment: algorithms, implementations, software environments, and the data ingested and execution logs of the computation. These two aspects of computational reproducibility (workflows and execution details) are discussed within the context of biomolecular Nuclear Magnetic Resonance spectroscopy (bioNMR), as well as the PRIMAD model for computational reproducibility.{$<$}/p{$>$}},
  language = {en},
  number = {4},
  urldate = {2018-02-11},
  journal = {Library Trends},
  doi = {10.1353/lib.2017.0018},
  url = {https://muse.jhu.edu/article/669457},
  author = {Gryk, Michael R. and Lud{\"a}scher, Bertram},
  month = sep,
  year = {2017},
  pages = {555-562},
  file = {/Users/ludaesch/Dropbox/zotero/storage/MVVUM8M6/Gryk and Ludäscher - 2017 - Workflows and Provenance Toward Information Scien.pdf}
}

@incollection{rauber16primad,
  address = {{Leibniz-Zentrum f{\"u}r Informatik, Schloss Dagstuhl, Germany}},
  title = {{{PRIMAD}}: {{Information}} Gained by Different Types of Reproducibility},
  volume = {6},
  booktitle = {Reproducibility of {{Data}}-{{Oriented Experiments}} in e-{{Science}} ({{Dagstuhl Seminar}} 16041)},
  publisher = {{Dagstuhl Publishing}},
  author = {Rauber, Andreas and Braganholo, Vanessa and Dittrich, Jens and Ferro, Nicola and Freire, Juliana and Fuhr, Norbert and Garijo, Daniel and Goble, Carole and J{\"a}\"rvelin, Kalervo and Lud{\"a}scher, Bertram and Stein, Benno and Stotzka, Rainer},
  year = {2016},
  pages = {128-132},
  file = {/Users/ludaesch/Dropbox/zotero/storage/S83LRKFZ/Rauber et al. - 2016 - PRIMAD Information gained by different types of r.pdf},
  crossref = {freire2016reproducibilitya}
}

@article{ferro2016increasing,
  title = {Increasing {{Reproducibility}} in {{IR}}: {{Findings}} from the {{Dagstuhl Seminar}} on "{{Reproducibility}} of {{Data}}-{{Oriented Experiments}} in e-{{Science}}"},
  volume = {50},
  issn = {0163-5840},
  shorttitle = {Increasing {{Reproducibility}} in {{IR}}},
  abstract = {The Dagstuhl Seminar on "Reproducibility of Data-Oriented Experiments in e-Science", held on 24-29 January 2016, focused on the core issues and approaches to reproducibility of experiments from a multidisciplinary point of view, sharing the experience coming from several fields of computer science. In this paper, we discuss, summarize, and adapt the main findings of the seminar to the context of IR evaluation -- both system-oriented and user-oriented -- in order to raise awareness in our community and stimulate the fields towards and increased reproducibility of our experiments.},
  number = {1},
  urldate = {2018-03-15},
  journal = {SIGIR Forum},
  doi = {10.1145/2964797.2964808},
  url = {http://doi.acm.org/10.1145/2964797.2964808},
  author = {Ferro, Nicola and Fuhr, Norbert and J{\"a}rvelin, Kalervo and Kando, Noriko and Lippold, Matthias and Zobel, Justin},
  month = jun,
  year = {2016},
  keywords = {PRIMAD},
  pages = {68--82},
  file = {/Users/ludaesch/Dropbox/zotero/storage/SWIFEU9D/Ferro et al. - 2016 - Increasing Reproducibility in IR Findings from th.pdf}
}

@article{chapp2019applicability,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.05211},
  primaryClass = {astro-ph},
  title = {Applicability Study of the {{PRIMAD}} Model to {{LIGO}} Gravitational Wave Search Workflows},
  abstract = {The PRIMAD model with its six components (i.e., Platform, Research Objective, Implementation, Methods, Actors, and Data), provides an abstract taxonomy to represent computational experiments and enforce reproducibility by design. In this paper, we assess the model applicability to a set of Laser Interferometer Gravitational-Wave Observatory (LIGO) workflows from literature sources (i.e., published papers). Our work outlines potentials and limits of the model in terms of its abstraction levels and application process.},
  urldate = {2019-06-20},
  journal = {arXiv:1904.05211 [astro-ph]},
  url = {http://arxiv.org/abs/1904.05211},
  author = {Chapp, Dylan and Rorabaugh, Danny and Brown, Duncan and Deelman, Ewa and Vahi, Karan and Welch, Von and Taufer, Michela},
  month = apr,
  year = {2019},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/Users/ludaesch/Dropbox/zotero/storage/I4DT42EM/Chapp et al. - 2019 - Applicability study of the PRIMAD model to LIGO gr.pdf;/Users/ludaesch/Dropbox/zotero/storage/5MPFDUY3/1904.html}
}



@inproceedings{drummond2009replicability,
  address = {{Montreal}},
  title = {Replicability Is Not {{Reproducibility}}: {{Nor}} Is It {{Good Science}}},
  shorttitle = {Replicability Is Not {{Reproducibility}}},
  abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount.  I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two.  Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
  urldate = {2017-08-14},
  booktitle = {Proc. {{Evaluation Methods}} for {{Machine Learning Workshop}} at the 26th {{ICML}}},
  publisher = {{National Research Council of Canada}},
  url = {http://cogprints.org/7691/},
  author = {Drummond, Dr Chris},
  month = jun,
  year = {2009},
  file = {/Users/ludaesch/Dropbox/zotero/storage/MDVHD6GH/Drummond - 2009 - Replicability is not Reproducibility Nor is it Go.pdf;/Users/ludaesch/Dropbox/zotero/storage/52KAWHNT/7691.html}
}


@article{plesser2018reproducibility,
  title = {Reproducibility vs. {{Replicability}}: {{A Brief History}} of a {{Confused Terminology}}},
  volume = {11},
  issn = {1662-5196},
  shorttitle = {Reproducibility vs. {{Replicability}}},
  abstract = {Reproducibility vs. Replicability: A Brief History of a Confused Terminology},
  language = {English},
  urldate = {2018-01-27},
  journal = {Frontiers in Neuroinformatics},
  doi = {10.3389/fninf.2017.00076},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076/full},
  author = {Plesser, Hans E.},
  year = {2018},
  keywords = {reproducibility,artifacts,computational science,repeatability,replicability},
  file = {/Users/ludaesch/Dropbox/zotero/storage/KIME55QQ/Plesser - 2018 - Reproducibility vs. Replicability A Brief History.pdf}
}

@article{goodman2016what,
  title = {What Does Research Reproducibility Mean?},
  volume = {8},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  issn = {1946-6234, 1946-6242},
  abstract = {The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''
The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences.
The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences.},
  language = {en},
  number = {341},
  urldate = {2018-01-27},
  journal = {Science Translational Medicine},
  doi = {10.1126/scitranslmed.aaf5027},
  url = {http://stm.sciencemag.org/content/8/341/341ps12},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  month = jun,
  year = {2016},
  file = {/Users/ludaesch/Dropbox/zotero/storage/VAFJ3RXY/Goodman et al. - 2016 - What does research reproducibility mean.pdf;/Users/ludaesch/Dropbox/zotero/storage/WJUT2ZS6/341ps12.html},
  pmid = {27252173}
}

@article{barba2018terminologies,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03311},
  primaryClass = {cs},
  title = {Terminologies for {{Reproducible Research}}},
  abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
  urldate = {2018-10-11},
  journal = {arXiv:1802.03311 [cs]},
  url = {http://arxiv.org/abs/1802.03311},
  author = {Barba, Lorena A.},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Digital Libraries},
  file = {/Users/ludaesch/Dropbox/zotero/storage/9A4Q46XJ/Barba - 2018 - Terminologies for Reproducible Research.pdf;/Users/ludaesch/Dropbox/zotero/storage/7FD3S67Q/1802.html;/Users/ludaesch/Dropbox/zotero/storage/BHEQ8K98/1802.html}
}

@techreport{herouxtoward,
  address = {{Albuquerque, New Mexico}},
  title = {Toward a {{Compatible Reproducibility Taxonomy}} for {{Computational}} and {{Computing Sciences}}},
  number = {SAND2018-11186},
  institution = {{Sandia National Laboratories}},
  author = {Heroux, Michael A and Barba, Lorena A and Parashar, Manish and Stodden, Victoria and Taufer, Michela},
  month = oct,
  year = {2018},
  file = {/Users/ludaesch/Dropbox/zotero/storage/EVWVGWPA/Heroux et al. - 2018 - Toward a Compatible Reproducibility Taxonomy for C.pdf}
}

@incollection{SMALHEISER20173,
  title = {Chapter 1 - {{Reproducibility}} and {{Robustness}}},
  isbn = {978-0-12-811306-6},
  booktitle = {Data {{Literacy}}},
  publisher = {{Academic Press}},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128113066000014},
  author = {Smalheiser, Neil R.},
  editor = {Smalheiser, Neil R.},
  year = {2017},
  keywords = {Conclusion,Effect size,Findings,Generalizability,Proxy,Replication,Reproducible,Robust,Statistically significant difference,Validity},
  pages = {3-15},
  file = {/Users/ludaesch/Dropbox/zotero/storage/8PHPRGMC/Smalheiser - 2017 - Chapter 1 - Reproducibility and Robustness.pdf},
  doi = {https://doi.org/10.1016/B978-0-12-811306-6.00001-4}
}

@article{ioannidis2017reproducibility,
  title = {The {{Reproducibility Wars}}: {{Successful}}, {{Unsuccessful}}, {{Uninterpretable}}, {{Exact}}, {{Conceptual}}, {{Triangulated}}, {{Contested Replication}}},
  volume = {63},
  copyright = {\textcopyright{} 2017 American Association for Clinical Chemistry},
  issn = {0009-9147, 1530-8561},
  shorttitle = {The {{Reproducibility Wars}}},
  abstract = {The recent publication of first results from the ``Reproducibility Project: Cancer Biology'' has stirred debate. The project synopsis put together by Nosek and Errington (1) tried to describe carefully what replication means, how to judge whether ``same'' (or different) results emerge in a replication experiment, and how to interpret divergent results in original vs reproducibility studies. However, multiple other commentators on these first reproducibility studies have enhanced our uncertainty. Almost every commentator reached a somewhat different conclusion. Reproducibility of inferences has been dismal.

Several authors of the original papers along with other commentators have questioned the reproducibility effort. These retorts typically defend the original findings, interpreting the replications as more successful than unsuccessful. They question replications on multiple fronts, e.g., inappropriate statistical methods or poor experimental competence. They lament the inappropriate shaming consequences, when poorly done replication efforts tarnish great scientists. They worry that reproducibility checks destroy discovery and stall efforts to translate promising research. They wonder whether we should waste money on replication.

The reproducibility wars are not exclusive to laboratory science. Last year, a similar debate erupted with a Technical Comment exchange on the respective reproducibility project on psychology (2), although the available data were far more extensive (100 experiments instead of just 5 preliminary ones) and had a massive participation of top psychologists (270 scientists and their teams) trying to reproduce results. Some famous psychology academics nevertheless concluded that their field had no reproducibility problem and reproducibility was misleading business. This position is immediately suspect. If psychological science is so perfect, how can it be that 270 of the best psychologists in the world working under optimal conditions of openness and most rigorous protocols and methods could get everything so badly wrong? If the most closely controlled and scrutinized corpus of experiments ever done on psychological science \ldots{}},
  language = {en},
  number = {5},
  urldate = {2019-02-24},
  journal = {Clinical Chemistry},
  doi = {10.1373/clinchem.2017.271965},
  url = {http://clinchem.aaccjnls.org/content/63/5/943},
  author = {Ioannidis, John P. A.},
  month = may,
  year = {2017},
  pages = {943-945},
  file = {/Users/ludaesch/Dropbox/zotero/storage/JEB2Y54G/Ioannidis - 2017 - The Reproducibility Wars Successful, Unsuccessful.pdf;/Users/ludaesch/Dropbox/zotero/storage/K43UMRK8/943.html},
  pmid = {28298413}
}

@inproceedings{stodden2013setting,
  title = {Setting the {{Default}} to {{Reproducible}}: {{Reproducibility}} in {{Computational}} and {{Experimental Mathematics}}},
  abstract = {Science is built upon foundations of theory and experiment validated and improved through open, trans- parent communication. With the increasingly central role of computation in scientific discovery this means communicating all details of the computations needed for others to replicate the experiment, i.e. making avail- able to others the associated data and code. The ``reproducible research'' movement recognizes that traditional scientific research and publication practices now fall short of this ideal, and encourages all those involved in the production of computational science \textendash{} scientists who use computational methods and the institutions that employ them, journals and dissemination mechanisms, and funding agencies \textendash{} to facilitate and practice really reproducible research.
This report summarizes discussions that took place during the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics, held December 10-14, 2012. The main recommendations that emerged from the workshop discussions are:
1. It is important to promote a culture change that will integrate computational reproducibility into the research process.
2. Journals, funding agencies, and employers should support this culture change.
3. Reproducibleresearchpracticesandtheuseofappropriatetoolsshouldbetaughtasstandardoperat- ing procedure in relation to computational aspects of research.
The workshop discussions included presentations of a number of the diverse and rapidly growing set of soft- ware tools available to aid in this effort. We call for a broad implementation of these three recommendations across the computational sciences.},
  booktitle = {{{ICERM Workshop}} on {{Reproducibility}} in {{Computational}} and {{Experimental Mathematics}}},
  editor = {Stodden, Victoria and Bailey, David H. and Borwein, Jonathan and LeVeque, Randall and Rider, Bill and Stein, William},
  month = feb,
  year = {2013},
  file = {/Users/ludaesch/Dropbox/zotero/storage/DTMLJ8WZ/Stodden et al. - 2013 - Setting the Default to Reproducible Reproducibili.pdf}
}

@article{freire2016reproducibilitya,
  title = {Reproducibility of {{Data}}-{{Oriented Experiments}} in e-{{Science}} ({{Dagstuhl Seminar}} 16041)},
  volume = {6},
  issn = {2192-5283},
  number = {1},
  urldate = {2019-05-20},
  journal = {Dagstuhl Reports},
  doi = {10.4230/DagRep.6.1.108},
  url = {http://drops.dagstuhl.de/opus/volltexte/2016/5817},
  author = {Freire, Juliana and Fuhr, Norbert and Rauber, Andreas},
  editor = {Freire, Juliana and Fuhr, Norbert and Rauber, Andreas},
  year = {2016},
  keywords = {Documentation,Reliability,Repeatibility,Replicability,Software,reproducibility},
  pages = {108--159},
  file = {/Users/ludaesch/Dropbox/zotero/storage/2ZQ95PMP/Freire et al. - 2016 - Reproducibility of Data-Oriented Experiments in e-.pdf;/Users/ludaesch/Dropbox/zotero/storage/XAKFY4V5/Freire et al. - 2016 - Reproducibility of Data-Oriented Experiments in e-.pdf;/Users/ludaesch/Dropbox/zotero/storage/2JYU7WAJ/5817.html;/Users/ludaesch/Dropbox/zotero/storage/I5UN9D66/5817.html}
}

@book{committeeonreproducibilityandreplicabilityinscience2019reproducibility,
  address = {{Washington, DC}},
  title = {\href{https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science}{Reproducibility and {{Replicability}} in {{Science}}}},
  isbn = {978-0-309-48613-2},
  abstract = {One of the pathways by which the scientific community confirms the validity of a new scientific discovery is by repeating the research that produced it. When a scientific effort fails to independently confirm the computations or results of a previous study, some fear that it may be a symptom of a lack of rigor in science, while others argue that such an observed inconsistency can be an important precursor to new discovery. about reproducibility and replicability have been expressed in both scientific and popular media. As these concerns came to light, Congress requested that the National Academies of Sciences, Engineering, and Medicine conduct a study to assess the extent of issues related to reproducibility and replicability and to offer recommendations for improving rigor and transparency in scientific research. and Replicability in Science defines reproducibility and replicability and examines the factors that may lead to non-reproducibility and non-replicability in research. Unlike the typical expectation of reproducibility between two computations, expectations about replicability are more nuanced, and in some cases a lack of replicability can aid the process of scientific discovery. This report provides recommendations to researchers, academic institutions, journals, and funders on steps they can take to improve reproducibility and replicability in science.},
  publisher = {{The National Academies Press}},
  url = {https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science},
  author = {{Committee on Reproducibility {and} Replicability in Science}},
  year = {2019},
  file = {/Users/ludaesch/Dropbox/zotero/storage/WSUYIPPS/National Academies of Sciences and Medicine - 2019 - Reproducibility and Replicability in Science.pdf},
  doi = {10.17226/25303}
}


@misc{carolegoble2016what,
  title = {What Is {{Reproducibility}}? {{The R}}* {{Brouhaha}}},
  author = {Carole Goble},
  note = {Symposium Reproducibility, Sustainability and Preservation, Alan Turing Institute}, 
  address = {{Alan Turing Institute}},
  copyright = {License: CC Attribution License},
  urldate = {2019-07-05},
  url = {https://www.slideshare.net/carolegoble/what-is-reproducibility-gobleclean},
  month = apr,
  year = 2016,
}

@article{chen2019open,
  title = {Open Is Not Enough},
  volume = {15},
  copyright = {2018 Springer Nature Limited},
  issn = {1745-2481},
  abstract = {The solutions adopted by the high-energy physics community to foster reproducible research are examples of best practices that could be embraced more widely. This first experience suggests that reproducibility requires going beyond openness.},
  language = {En},
  number = {2},
  urldate = {2019-07-05},
  journal = {Nature Physics},
  doi = {10.1038/s41567-018-0342-2},
  url = {https://www.nature.com/articles/s41567-018-0342-2},
  author = {Chen, Xiaoli and {Dallmeier-Tiessen}, S{\"u}nje and Dasler, Robin and Feger, Sebastian and Fokianos, Pamfilos and Gonzalez, Jose Benito and Hirvonsalo, Harri and Kousidis, Dinos and Lavasa, Artemis and Mele, Salvatore and Rodriguez, Diego Rodriguez and {\v S}imko, Tibor and Smith, Tim and Trisovic, Ana and Trzcinska, Anna and Tsanaktsidis, Ioannis and Zimmermann, Markus and Cranmer, Kyle and Heinrich, Lukas and Watts, Gordon and Hildreth, Michael and Iglesias, Lara Lloret and {Lassila-Perini}, Kati and Neubert, Sebastian},
  month = feb,
  year = {2019},
  pages = {113},
  file = {/Users/ludaesch/Dropbox/zotero/storage/W3JKWUFP/Chen et al. - 2019 - Open is not enough.pdf;/Users/ludaesch/Dropbox/zotero/storage/MZY6278T/s41567-018-0342-2.html}
}

@article{pimentel2019survey,
  title = {A {{Survey}} on {{Collecting}}, {{Managing}}, and {{Analyzing Provenance}} from {{Scripts}}},
  volume = {52},
  issn = {0360-0300},
  abstract = {Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.},
  number = {3},
  urldate = {2019-06-26},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/3311955},
  url = {http://doi.acm.org/10.1145/3311955},
  author = {Pimentel, Jo{\~a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa},
  month = jun,
  year = {2019},
  keywords = {Provenance,analyzing,collecting,managing,scripts,survey},
  pages = {47:1--47:38},
  file = {/Users/ludaesch/Dropbox/zotero/storage/CBANMMMY/Pimentel et al. - 2019 - A Survey on Collecting, Managing, and Analyzing Pr.pdf}
}


@article{schwab2000making,
  title = {Making Scientific Computations Reproducible},
  volume = {2},
  issn = {1521-9615},
  abstract = {To verify a research paper's computational results, readers typically have to recreate them from scratch. ReDoc is a simple software filing system for authors that lets readers easily reproduce computational results using standardized rules and commands.},
  number = {6},
  journal = {Computing in Science Engineering},
  doi = {10.1109/5992.881708},
  author = {Schwab, M. and Karrenbach, N. and Claerbout, J.},
  month = nov,
  year = {2000},
  keywords = {Documentation,natural sciences computing,Software testing,Laboratories,authors,computational results,Computer interfaces,document handling,Electronic publishing,file organisation,Organizing,ReDoc,Reproducibility of results,reproducible scientific computations,research and development management,research paper,software filing system,Software maintenance,Software systems,standardized rules,Technological innovation},
  pages = {61-67},
  file = {/Users/ludaesch/Dropbox/zotero/storage/KPBPKF2K/Schwab et al. - 2000 - Making scientific computations reproducible.pdf;/Users/ludaesch/Dropbox/zotero/storage/7AK543WT/881708.html}
}

@incollection{claerbout1992electronic,
  title = {Electronic Documents Give Reproducible Research a New Meaning},
  booktitle = {{{SEG Technical Program Expanded Abstracts}} 1992},
  publisher = {{Society of Exploration Geophysicists}},
  author = {Claerbout, Jon F and Karrenbach, Martin},
  year = {1992},
  pages = {601-604}
}


@misc{acm2018artifact,
  title = {Artifact {{Review}} and {{Badging}}},
  abstract = {Result and Artifact Review and Badging},
  language = {en},
  urldate = {2019-07-05},
  note = {\url{https://www.acm.org/publications/policies/artifact-review-badging}},
  author = {ACM},
  month = apr,
  year = {2018},
  file = {/Users/ludaesch/Dropbox/zotero/storage/VNSH7T5G/artifact-review-badging.html}
}



@article{milkowski2018replicability,
  title = {Replicability or Reproducibility? {{On}} the Replication Crisis in Computational Neuroscience and Sharing Only Relevant Detail},
  volume = {45},
  issn = {1573-6873},
  shorttitle = {Replicability or Reproducibility?},
  abstract = {Replicability and reproducibility of computational models has been somewhat understudied by ``the replication movement.'' In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code.},
  language = {en},
  number = {3},
  urldate = {2019-07-05},
  journal = {Journal of Computational Neuroscience},
  doi = {10.1007/s10827-018-0702-z},
  url = {https://doi.org/10.1007/s10827-018-0702-z},
  author = {Mi{\l}kowski, Marcin and Hensel, Witold M. and Hohol, Mateusz},
  month = dec,
  year = {2018},
  keywords = {Computational modeling,Direct and conceptual replication,Methodology of computational neuroscience,Replication and reproduction,Replication studies},
  pages = {163-172},
  file = {/Users/ludaesch/Dropbox/zotero/storage/JWIPGTCF/Miłkowski et al. - 2018 - Replicability or reproducibility On the replicati.pdf}
}



@inproceedings{chard2019,
 author = {Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Lud\"{a}scher, Bertram and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Turk, Matthew J. and Willis, Craig},
 title = {Implementing Computational Reproducibility in the Whole Tale Environment},
 booktitle = {Proceedings of the 2Nd International Workshop on Practical Reproducible Evaluation of Computer Systems},
 series = {P-RECS '19},
 year = {2019},
 isbn = {978-1-4503-6756-1},
 location = {Phoenix, AZ, USA},
 pages = {17--22},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3322790.3330594},
 doi = {10.1145/3322790.3330594},
 acmid = {3330594},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computing environments, cyberinfrastructure, open code, open data, publishing standards, reproducibility},
} 


@Misc{WT2019,
  key = 	 {WT},
  title = 	 {Whole {T}ale Project},
  howpublished = {\url{https://wholetale.org/}},
  year = 	 2019}



@article{bechhofer2013whya,
  series = {Special Section: {{Recent}} Advances in e-{{Science}}},
  title = {Why Linked Data Is Not Enough for Scientists},
  volume = {29},
  issn = {0167-739X},
  abstract = {Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.},
  number = {2},
  urldate = {2019-06-25},
  journal = {Future Generation Computer Systems},
  doi = {10.1016/j.future.2011.08.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0167739X11001439},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  month = feb,
  year = {2013},
  keywords = {Publishing,Reproducibility,Linked data,Research object,Reuse,Sharing},
  pages = {599-611},
  file = {/Users/ludaesch/Dropbox/zotero/storage/S3ND4232/Bechhofer et al. - 2013 - Why linked data is not enough for scientists.pdf;/Users/ludaesch/Dropbox/zotero/storage/3R28L5KE/S0167739X11001439.html}
}


@book{kuhn1962structure,
  title = {The Structure of Scientific Revolutions},
  publisher = {{University of Chicago Press}},
  author = {Kuhn, Thomas},
  year = {1962}
}


@proceedings{DBLP:conf/eScience/2018,
  title     = {14th {IEEE} International Conference on e-Science, e-Science 2018,
               Amsterdam, The Netherlands, October 29 - November 1, 2018},
  publisher = {{IEEE} Computer Society},
  key = {RO2018}, 
  year      = {2018},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8588285},
  isbn      = {978-1-5386-9156-4},
  timestamp = {Fri, 04 Jan 2019 13:57:07 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/eScience/2018},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{mecum2018preservingc,
  title = {Preserving {{Reproducibility}}: {{Provenance}} and {{Executable Containers}} in {{DataONE Data Packages}}},
  shorttitle = {Preserving {{Reproducibility}}},
  abstract = {Many data packaging standards are available to researchers and data repository operators and the choice to use an existing standard or create a new one is challenging. We introduce the DataONE Data Package standard which is based on the existing OAI-ORE Resource Map standard. We describe the functionality Data Package provides, implementation considerations, compare it to existing standards, and discuss future extensions to the standard including the ability to describe execution environments via WholeTale "Tales"" and alternate serialization formats.},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  doi = {10.1109/eScience.2018.00019},
  author = {Mecum, B. and Jones, M. B. and Vieglais, D. and Willis, C.},
  month = oct,
  year = {2018},
  keywords = {data handling,data packaging,data packaging standards,data repository operators,DataONE,DataONE data package standard,executable containers,Interoperability,Metadata,OAI-ORE,OAI-ORE resource map standard,Packaging,preserving reproducibility,reproducibility,Resource description framework,Software,standards,Standards,WholeTale},
  pages = {45-49},
  file = {/Users/ludaesch/Dropbox/zotero/storage/F22TXB4Q/8588638.html}
}


@techreport{FASEB2016enhancing,
  title = {\href{http://faseb.org/Resources-for-the-Public/News-Room/Article-Detail-View/tabid/1014/ArticleId/1251/FASEB-Issues-Recommendations-on-Reproducibility.aspx}{Enhancing {{Research Reproducibility}}: {{Recommendations}} from the {{Federation}} of {{American Societies}} for {{Experimental Biology}}}},
  author = {FASEB},
  year = {2016},
  file = {/Users/ludaesch/Dropbox/zotero/storage/XKPIZAFY/FASEB - 2016 - Enhancing Research Reproducibility Recommendation.pdf}
}



@misc{groth2013provoverviewa,
  type = {Monograph},
  title = {{{PROV}}-{{Overview}}. {{An Overview}} of the {{PROV Family}} of {{Documents}}},
  abstract = {Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. The PROV Family of Documents defines a model, corresponding serializations and other supporting definitions to enable the inter-operable interchange of provenance information in heterogeneous environments such as the Web. This document provides an overview of this family of documents.},
  language = {English},
  urldate = {2019-07-05},
  note = {\url{http://www.w3.org/TR/2013/NOTE-prov-overview-20130430/}},
  author = {Groth, Paul and Moreau, Luc},
  collaborator = {Groth, Paul and Moreau, Luc},
  month = apr,
  year = {2013},
  file = {/Users/ludaesch/Dropbox/zotero/storage/7JG8KC7D/356854.html}
}



@inproceedings{mcphillips2015retrospective,
  title = {Retrospective Provenance without a Runtime Provenance Recorder},
  booktitle = {Proceedings of the 7th {{USENIX Conference}} on {{Theory}} and {{Practice}} of {{Provenance}}},
  author = {McPhillips, Timothy and Bowers, Shawn and Belhajjame, Khalid and Lud{\"a}scher, Bertram},
  year = {2015},
}


@article{fanelli_opinion:_2018,
	title = {Opinion: {Is} science really facing a reproducibility crisis, and do we need it to?},
	volume = {115},
	issn = {0027-8424},
	url = {https://www.pnas.org/content/115/11/2628},
	doi = {10.1073/pnas.1708272114},
	abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
	number = {11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fanelli, Daniele},
	year = {2018},
	pages = {2628--2631}
}

@article{munafo2018robust,
  title = {Robust Research Needs Many Lines of Evidence},
  volume = {553},
  copyright = {2018 Nature},
  abstract = {Replication is not enough. Marcus R. Munaf{\`o} and George Davey Smith state the case for triangulation.},
  language = {EN},
  number = {7689},
  urldate = {2019-07-06},
  journal = {Nature},
  doi = {10.1038/d41586-018-01023-3},
  url = {http://www.nature.com/articles/d41586-018-01023-3},
  author = {Munaf{\`o}, Marcus R. and Smith, George Davey},
  month = jan,
  year = {2018},
  pages = {399},
  file = {/Users/ludaesch/Dropbox/zotero/storage/68MMIHC4/Munafò and Smith - 2018 - Robust research needs many lines of evidence.pdf;/Users/ludaesch/Dropbox/zotero/storage/474HW8LB/d41586-018-01023-3.html}
}


@article{spenkelink_recycling_2019,
	title = {Recycling of single-stranded {DNA}-binding protein by the bacterial replisome},
	volume = {47},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/47/8/4111/5320382},
	doi = {10.1093/nar/gkz090},
	abstract = {Abstract.  Single-stranded DNA-binding proteins (SSBs) support DNA replication by protecting single-stranded DNA from nucleolytic attack, preventing intra-stran},
	language = {en},
	number = {8},
	urldate = {2019-07-06},
	journal = {Nucleic Acids Research},
	author = {Spenkelink, Lisanne M. and Lewis, Jacob S. and Jergic, Slobodan and Xu, Zhi-Qiang and Robinson, Andrew and Dixon, Nicholas E. and van Oijen, Antoine M.},
	month = may,
	year = {2019},
	pages = {4111--4123},
	file = {Full Text PDF:C\:\\Users\\tmcphill\\Zotero\\storage\\88D5CWUX\\Spenkelink et al. - 2019 - Recycling of single-stranded DNA-binding protein b.pdf:application/pdf;Snapshot:C\:\\Users\\tmcphill\\Zotero\\storage\\ECF72TX7\\5320382.html:text/html}
}


@article{robasky_role_2014,
	title = {The {Role} of {Replicates} for {Error} {Mitigation} in {Next}-{Generation} {Sequencing}},
	volume = {15},
	issn = {1471-0056},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4103745/},
	doi = {10.1038/nrg3655},
	abstract = {Advances in next-generation technologies have rapidly improved sequencing fidelity and significantly decreased sequencing error rates. However, with billions of nucleotides in a human genome, even low experimental error rates yield many errors in variant calls. Erroneous variants can mimic true somatic and rare variants, thus requiring costly confirmatory experiments to minimize the number of false positives. Here we discuss sources of experimental error in next-generation sequencing and how replicates can be used to abate them.},
	number = {1},
	urldate = {2019-07-06},
	journal = {Nature reviews. Genetics},
	author = {Robasky, Kimberly and Lewis, Nathan E. and Church, George M.},
	month = jan,
	year = {2014},
	pmid = {24322726},
	pmcid = {PMC4103745},
	pages = {56--62},
	file = {PubMed Central Full Text PDF:C\:\\Users\\tmcphill\\Zotero\\storage\\XAGDAW7K\\Robasky et al. - 2014 - The Role of Replicates for Error Mitigation in Nex.pdf:application/pdf}
}




