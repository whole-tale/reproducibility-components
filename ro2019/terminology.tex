\section{Terminology}\label{sec-terminology}

What are some specific ways that Research Objects \cite{bechhofer2013whya} can help
	make scientific research more transparent?
Many of the objectives and current capabilities of Research Objects already can be seen as supporting
	transparency~\cite{mecum2018preservingc}.
In the remainder of this paper we propose that Research Objects can help in additional ways that not
	just enhance the transparency of research, but also ensure that transparency and other key elements
	of scientific reproducibility can be achieved, described, and shared meaningfully for all domains
	of research---including those that include both experimental and computational elements.

The first way in which Research Objects can help is by helping researchers safely navigate the
	terminological quagmire surrounding the definitions of terms such as \emph{reproducible},
	\emph{replicable}, and \emph{transparent}.
A very simple yet important use case for Research Objects (ROs) could be the declaration of the senses in
	which the research study and results associated with the RO are in fact reproducible, replicable,
	computationally repeatable, and so on.
Before extending or depending on others' works, methods, or results in their own studies, researchers
	reasonably want to know if that previous work is reproducible in various senses of the word.
ROs can help, not just be providing a place to make such declarations, but by preventing
	misunderstandings of what is meant by particular terms.

The current debate over the meaning of key terms describing
	scientific reproducibility is motivated primarily by a desire to avoid just such confusion \cite{drummond2009replicability,goodman2016what,rauber16primad,herouxtoward,plesser2018reproducibility,barba2018terminologies,committeeonreproducibilityandreplicabilityinscience2019reproducibility}.
The recommendations from the Federation of
	American Societies for Experimental Biology\footnote{FASEB is a federation of thirty distinct
    scientific societies representing over 130,000 researchers in the biological sciences~\cite{faseb_home}.}
	(FASEB)
	\cite{FASEB2016enhancing} cite ``lack of uniform definitions to describe the problem''
	as one of the top three factors that ``impede the ability to reproduce experimental results.''
 The recent report from the National Academy of Sciences (NAS) Committee on Reproducibility and Replicability of Science \cite{committeeonreproducibilityandreplicabilityinscience2019reproducibility} asserts
	that ``the difficulties in assessing reproducibility and replicability are complicated by this absence of
	standard definitions for these terms.''

The recommendations from these two organizations are representative of numerous recent studies, papers,
	and proposed definitions intended to enhance reproducibility by providing a uniform terminology
	for describing it.
The FASEB recommendations originate in one domain of science while the NAS definitions explicitly
	``are intended to apply across all fields of science.''
Given the interdisciplinary character of modern research---and in particular the ubiquity of computing in science---it
	is hard to argue against attempts to facilitate communication about reproducibility across science as a whole.

What can be surprising to researchers new to this debate is how many ways the proposed definitions
	can differ.
First, there is disagreement over which term, \emph{reproducibility} or \emph{replicability}, indicates
	 a greater adherence to the procedures, material,  and methods employed in the original research.
The FASEB definitions\footnote{In accordance with the terminology around \emph{replicates} described in Section\,\ref{sec-reproducibility}.}
	require from \emph{replicability} a greater fidelity to the original study \cite[p.3]{FASEB2016enhancing}:
        \begin{quote}
          \textbf{Replicability}: the ability to duplicate (i.e., repeat) a prior result using the same
          source materials and methodologies. This term should only be used when
	referring to repeating the results of a specific experiment rather than an
	entire study. \medskip

	\textbf{Reproducibility}: the ability to achieve similar or nearly identical results using comparable materials and methodologies.
	This term may be used when specific findings from a study are obtained by an independent group of researchers.
      \end{quote}
According to FASEB, \emph{replicability} indicates a higher degree of fidelity than does \emph{reproducibility},
	both with respect to the prior result to be confirmed, and to the materials and methodologies employed.
Replicability also appears likely more feasible for the original researchers (they presumably have access to the
	``same source materials'' and are in the best position to use the ``same methodologies''), whereas reproducibility is
	feasible for ``an independent group of researchers''.
Both definitions may be applied to experimental results, but neither definition precludes application to \emph{in silico}
	experiments or to the computational elements of laboratory studies.

 In contrast, the definitions in the report from the National Academy of Sciences reverses the relative
	 fidelity implied by the terms `reproducibility' and `replicability' \cite[p.4]{committeeonreproducibilityandreplicabilityinscience2019reproducibility}:
\begin{quote}
	\textbf{Reproducibility} is obtaining consistent results using the same input data, computational
	steps, methods, and code, and conditions of analysis.  \medskip

	\textbf{Replicability} is obtaining consistent results across studies aimed at answering the same
	scientific question, each of which has obtained its own data.
\end{quote}

The NAS definition of \emph{replicability} is most similar to the FASEB definition of \emph{reproducibility}.
The reversal of the meanings of these terms between various research domains is well documented within the NAS report,
	which in turn depends on Barba's comprehensive study of the terms~\cite{barba2018terminologies}.

This aspect of the disagreement over terminologies is in a sense trivial\footnote{
	The NAS report points out that the words \emph{reproducibility} and \emph{replicability} are
		``interchangeable in everyday discourse.''
	We note, however, that both the high-fidelity \emph{replication} of DNA (in the \emph{replisome}~\cite{spenkelink_recycling_2019})
		and the lower-fidelity \emph{reproduction}
		of organisms \emph{are} matters of everyday discourse for researchers who study these processes in nature or employ them in the lab.
	Furthermore, we observe a clear analogy between the exacting replication of DNA and careful replication of measurements and samples
		in the lab on the one hand; and on the other hand between the reproduction of organisms where variation is encouraged in nature
		(for example through sex) and the reproduction of scientific results across studies where, again, some variation is both
		expected and desirable.
    }, although the NAS likely is correct in
	asserting that the ``different meanings and uses across science and engineering'' has ``led to confusion in collectively
	understanding problems in reproducibility and replicability.''
Far more notably, the NAS report does not suggest new terms for referring to the \emph{technical replicates}
	and  \emph{biological replicates} so important in experimental biology--should biologists adopt the recommendation
	of restricting \emph{replication} to ``obtaining consistent results across studies''.\footnote{
		The NAS report section \emph{Precision of Measurement} quotes a portion of the International Vocabulary of
		Metrology that twice employs the term \emph{replicate measurement}.
	}

An even more intriguing aspect of the NAS definitions \cite{committeeonreproducibilityandreplicabilityinscience2019reproducibility}
	is that experiments not carried out
	entirely \emph{in silico} apparently are left only with the term \emph{replicability}.
Satisfying the NAS definition of \emph{reproducibility} requires
	``computational steps'' and ``code''.  The report goes on to clarify
	that reproducibility ``is synonymous with computational reproducibility,''  and ``the terms are used interchangeably in this report.''
Indeed the executive summary of the report states not only that ``we define reproducibility to mean computational reproducibility'',
	but also that ``the committee adopted definitions that are intended to apply across all fields of science.''
The clear implication is that the term \emph{reproducible} only can be applied to the computational components of research.
Because this term is analogous to \emph{replicable} as defined by FASEB, the NAS definitions do not provide a vocabulary
	that would enable experimentalists to report the intrinsic repeatability of their own non-computational methods, measurements, and results.

Note that by highlighting these aspects of the NAS definitions
	of reproducibility and replicability we are \emph{not} arguing
 	that the FASEB definitions of these terms are superior.
In particular, we do not propose that the latter definitions be adopted universally instead.
On the contrary, we suggest that the differences in the content of these (and the many other) definitions
of these two terms likely reflect specific, critical needs of the researchers and communities that adopt them.

Similarities and differences also appear in definitions and usages of the term \emph{transparency}.
According to FASEB \cite{FASEB2016enhancing}, \textbf{transparency} is:
\begin{quote}
	The reporting of experimental materials and methods in a manner that provides enough information
	for others to independently assess and/or reproduce experimental findings
      \end{quote}
while the NAS report \cite{committeeonreproducibilityandreplicabilityinscience2019reproducibility} states:
 \begin{quote}
	When a researcher transparently reports a study and makes available the underlying digital artifacts, such as data and code,
	the results should be computationally reproducible.
      \end{quote}

\noindent Again, the NAS usage of the term assumes that transparency is associated with digital artifacts.
This could be of concern to those expecting experimental procedures to be transparent as well.

What is more important, however, is what the two concepts of transparency have in common.
Both definitions imply that transparency is a desirable \emph{component} of scientific
    reproducibility in the broader sense of the term.
This shared insight suggests a role for Research Objects to play in the resolution of this
    terminological conundrum.
In short, we propose that (1) ROs \emph{provide users with vocabularies for asserting and
    querying the reproducibility of studies, results, and methods along multiple dimensions};
    and (2) that we \emph{enable users interacting with ROs to employ terminologies intuitive to
    researchers in their own communities}.
Namespaces would support multiple definitions of terms without conflict.
Synonym relationships and other mappings between the vocabularies would enable reasoning about
    reproducibility and support assertions and queries phrased using terminologies selected by
    the user.

    For example, a researcher publishing an RO might assert that the
    study is reproducible \emph{sensu} Whole Tale.  Another researcher
    filtering discovered ROs by the property \term{NAS::reproducible}
    would find this study either if \term{WT::reproducible} had been
    found to imply \term{NAS::reproducible} generally, or if other
    assertions made by the author about the RO satisfy the
    requirements of the latter term in conjunction with the
    implications of \term{WT::reproducible}.

The same approach could be applied to ROs published by communities adopting specific
    procedures for evaluating and verifying computational artifacts.
For example, the American Journal of Political Science (AJPS) has adopted a policy of
    verifying quantitative research \cite{christian2018}.
ACM SIGMOD has defined procedures for assessing database research
    reproducibility~\cite{bonnet_repeatability_2011, sigmod2018reproducibility}.
ROs deemed reproducible through these community-defined workflows could be
    identified as \term{AJPS::reproducible:v1.2}\footnote{
        The AJPS workflow is versioned. See for example
        \url{https://ajps.org/wp-content/uploads/2019/01/ajps-quant-data-checklist-ver-1-2.pdf}
    } and \term{SIGMOD::reproducible}, respectively.
The ACM additionally awards four different reproducibility badges~\cite{acm2018artifact} to publications
    with artifacts that meet particular criteria\footnote{The current guidelines for awarding
    badges provide links to seven past versions of the guidelines that have been used since 2015.}.
Given that a particular researcher is likely to depend on or extend results that have been
    assessed for reproducibility by different workflows (or different versions of those
    workflows), the ability to query the reproducibility of research products using the definitions and
    criteria of one's choosing is critical if these reproducibility assessment efforts
    are to have lasting value.

Capabilities for reasoning about definitions, verification workflows, and awarded
    badges in this way also would benefit emerging reproducibility platforms that
    target research with non-computational components.
Biomolecular nuclear magnetic resonance (bioNMR) spectroscopy is
    computationally intensive, with studies requiring dozens of software tools for data
    processing and analysis.
The \mbox{NMRbox} project~\cite{maciejewski_nmrbox:_2017} aims to foster reproducibility of such studies
    by providing researchers with access to
    Xubuntu virtual machines provisioned with more than 100 software tools used by this community.
Each VM image is versioned, stored in its entirety, and made
    available long-term to make the software environments, computations, and end-to-end workflows
    employed in a particular study computationally reproducible.
NMRbox uses PREMIS~\cite{premis2019} to represent and store provenance metadata,
    and packages this metadata with
    processed data and results to support transparency both of the analysis and of the NMR
    experiment itself~\cite{heintz_curating_2019}.
Queries probing the reproducibility of a particular result produced, archived, and shared
    using NMRbox necessarily would span claims made both about the (non-computational)
    experimental procedures employed, \emph{and} the computational analysis performed on
    the resulting experimental data.

Going forward, we aim to explore the various terminologies surrounding
    reproducibility with the goal of identifying what might be considered the
     principal components or perhaps even the \emph{dimensions} of scientific reproducibility\footnote{
        The idea that \emph{scientific reproducibility} in general might comprise multiple,
            independent dimensions
            is analogous to the claim made by the PRIMAD
            model~\cite{rauber16primad} that the information gained
            from a \emph{reproducibility study} is related to---and meaningful only in terms
            of---the ways in which it varies from the original study along
            different dimensions: platform, research objective, implementation, etc.
    }.
We could then determine how various terms and definitions,
    put forward to meet the needs of particular research communities,
    can be seen as compositions of these shared components.
This in turn would reveal how RO infrastructure should reason about these terms,
	and how claims made in terms of one set of definitions could be converted to
    claims using another set of definitions.
