\section{Reproducibility Queries}\label{sec-transparency}

Research Objects long have been advocated as vehicles for sharing the
	provenance of scientific results and data products~\cite{bechhofer2013whya}.
Here we list some ways we see extensions of our prior efforts in the area
	of science-oriented provenance queries
    enhancing reproducibility and transparency in
	Research Objects generally.

For provenance management systems, representations, and user interfaces to support
	scientific transparency, they must support science-oriented queries.
Provenance must address questions about the \emph{science} that was performed---not just the
	sequence, dependencies, and flow of data through computational steps.
The answers to these questions must enable others to evaluate the scientific quality of the work,
	and to learn what is necessary to
	reproduce the results \emph{without} actually repeating every step taken in the original work.
Provenance is key to enabling researchers to build on computed results reported in prior work with confidence.

For provenance to serve this function, however, it must be possible for researchers unversed in the detailed
	specifications of Research Objects and the PROV standard~\cite{groth2013provoverviewa} to pose
	questions and receive answers meaningful for evaluating, using, and building on the
	processes and products of prior research.
In \cite{mcphillips2015retrospective} we provided a number of example queries about a run of a scientific
	workflow implemented in Python and marked up with YesWorkflow~\cite{mcphillips2015yesworkflowa} annotations.
Answers to these queries revealed the transitive dependencies of particular workflow outputs
	on the experimental samples, instrument settings, and intermediate data products.
The queries were phrased in terms familiar to researchers in the example domain,
	and demonstrated how provenance queries can be used by scientists
	to answer scientific questions about research.

We plan to extend this approach to Research Objects that support the other capabilities
	proposed in this paper.
Such queries would allow a researcher to determine not just that a study as a whole
	is \term{FASEB::reproducible}, for example, but also that a particular result
	is \term{NAS::reproducible} (which is a completely different thing).
For studies that do not qualify as \term{FASEB::reproducible} as a whole, researchers
	could discover which results of the study are \term{FASEB::replicable}, and which are not.
Where a particular results is not \term{FASEB::replicable}, they could pose a query
	that reveals what part of the method that produced the result is
	not replicable.
Answers to such queries could take available technology for reproducing computations
	into account.
For example, a particular result might no longer be NAS::reproducible using the
	latest version of Docker.
